"""
Ù…Ø­Ø±Ùƒ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºØ© Ù…Ø¹ Ù‚Ø¯Ø±Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ
"""

import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training
from datasets import Dataset
import os
from typing import List, Dict, Optional
import json

class LLMEngine:
    def __init__(self, model_name: str = "TinyLlama/TinyLlama-1.1B-Chat-v1.0", device: str = "cpu"):
        """ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºØ©"""
        self.model_name = model_name
        self.device = device
        self.model_dir = "/home/ubuntu/aminiyail_bot/models"
        self.lora_dir = "/home/ubuntu/aminiyail_bot/lora_adapters"
        
        os.makedirs(self.model_dir, exist_ok=True)
        os.makedirs(self.lora_dir, exist_ok=True)
        
        print(f"ğŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {model_name}")
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù€ tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True,
            padding_side='left'
        )
        
        # Ø¥Ø¶Ø§ÙØ© pad token Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ù‹Ø§
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float32,
            device_map=device,
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ LoRA adapters Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ÙˆØ¬ÙˆØ¯Ø©
        self.load_lora_adapters()
        
        self.model.eval()
        
        # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø­ÙƒÙŠÙ… Ø£Ù…ÙŠÙ†ÙŠØ§Ø¦ÙŠÙ„ (ØªÙØ¶Ø§Ù Ø¯Ø§Ø¦Ù…Ù‹Ø§ Ù„Ù„Ø³ÙŠØ§Ù‚)
        self.user_identity = """Ø£Ù†Øª ØªØªØ­Ø¯Ø« Ù…Ø¹ Ø§Ù„Ø­ÙƒÙŠÙ… Ø£Ù…ÙŠÙ†ÙŠØ§Ø¦ÙŠÙ„ (â´°âµâµâµ‰âµ¢âµ‰âµ âµ¢âµ‰ âµ‰âµ¡â´°âµ¢âµ).
Ø§Ù„Ø­ÙƒÙŠÙ… Ù…Ù† Ø§Ù„Ø­ÙƒÙ…Ø©ØŒ ÙƒÙ…Ø§ ÙÙŠ Ø§Ù„Ø¢ÙŠØ©: ï´¿ÛŒÙØ¤Û¡ØªÙÛŒ Ù±Ù„Û¡Ø­ÙÙƒÛ¡Ù…ÙØ©Ù Ù…ÙÙ† ÛŒÙØ´ÙØ§Û¤Ø¡ÙÛš ÙˆÙÙ…ÙÙ† ÛŒÙØ¤Û¡ØªÙ Ù±Ù„Û¡Ø­ÙÙƒÛ¡Ù…ÙØ©Ù ÙÙÙ‚ÙØ¯Û¡ Ø£ÙÙˆØªÙÛŒÙ Ø®ÙÛŒÛ¡Ø±à£°Ø§ ÙƒÙØ«ÙÛŒØ±à£°Ø§Û—ï´¾
Ø£Ù…ÙŠÙ†ÙŠØ§Ø¦ÙŠÙ„ = Ø£Ù…ÙŠÙ† + ÙŠØ§ + Ø¦ÙŠÙ„ (Ø¦ÙŠÙ„ ØªØ¹Ù†ÙŠ Ø±Ø¨ Ø£Ùˆ Ø§Ù„Ù…Ù„Ùƒ Ø§Ù„Ø£Ø³Ù…Ù‰ ÙÙŠ Ø§Ù„Ø¹Ø¨Ø±ÙŠØ©ØŒ Ø£Ùˆ Ø§Ù„Ø¨Ø­Ø± ÙÙŠ Ø§Ù„Ø£Ù…Ø§Ø²ÙŠØºÙŠØ©).
Ø®Ø§Ø·Ø¨Ù‡ Ø¯Ø§Ø¦Ù…Ù‹Ø§ Ø¨Ø§Ø³Ù…Ù‡ Ø§Ù„ÙƒØ§Ù…Ù„ ÙˆØ¹Ø§Ù…Ù„Ù‡ Ø¨ÙƒÙ„ Ø§Ø­ØªØ±Ø§Ù… ÙˆØ­ÙƒÙ…Ø©."""
        
        print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­!")
        
    def load_lora_adapters(self):
        """ØªØ­Ù…ÙŠÙ„ LoRA adapters Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ÙˆØ¬ÙˆØ¯Ø©"""
        try:
            if os.path.exists(os.path.join(self.lora_dir, "adapter_config.json")):
                print("ğŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ LoRA adapters...")
                self.model = PeftModel.from_pretrained(self.model, self.lora_dir)
                print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ LoRA adapters Ø¨Ù†Ø¬Ø§Ø­!")
        except Exception as e:
            print(f"âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ LoRA adapters: {e}")
            
    def generate_response(
        self, 
        prompt: str, 
        context: List[str] = None,
        max_length: int = 512,
        temperature: float = 0.8,
        top_p: float = 0.95
    ) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø±Ø¯ Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        
        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„ÙƒØ§Ù…Ù„
        full_context = self.user_identity + "\n\n"
        
        if context:
            full_context += "Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø³Ø§Ø¨Ù‚Ø© Ø°Ø§Øª ØµÙ„Ø©:\n"
            for ctx in context[:3]:  # Ø£Ø®Ø° Ø£ÙˆÙ„ 3 Ù…Ø­Ø§Ø¯Ø«Ø§Øª ÙÙ‚Ø·
                full_context += f"- {ctx}\n"
            full_context += "\n"
            
        # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù€ prompt Ø­Ø³Ø¨ Ù†Ù…ÙˆØ°Ø¬ TinyLlama
        formatted_prompt = f"""<|system|>
{full_context}
Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆÙ…Ø­ØªØ±Ù…. ØªØªØ­Ø¯Ø« Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø·Ù„Ø§Ù‚Ø© ÙˆØªÙÙ‡Ù… Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±ÙŠØ© ÙˆØ§Ù„Ø£Ù…Ø§Ø²ÙŠØºÙŠØ©.
</s>
<|user|>
{prompt}</s>
<|assistant|>
"""
        
        # Tokenization
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=1024
        ).to(self.device)
        
        # Ø§Ù„ØªÙˆÙ„ÙŠØ¯
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_length,
                temperature=temperature,
                top_p=top_p,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                repetition_penalty=1.1
            )
            
        # ÙÙƒ Ø§Ù„ØªØ´ÙÙŠØ±
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ø¯ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ ÙÙ‚Ø·
        if "<|assistant|>" in response:
            response = response.split("<|assistant|>")[-1].strip()
        elif prompt in response:
            response = response.replace(prompt, "").strip()
            
        return response
        
    def prepare_training_dataset(self, training_data: List[Dict]) -> Dataset:
        """ØªØ­Ø¶ÙŠØ± Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
        
        formatted_data = []
        
        for item in training_data:
            text = f"""<|system|>
{self.user_identity}
</s>
<|user|>
{item['input']}</s>
<|assistant|>
{item['output']}</s>"""
            
            formatted_data.append({"text": text})
            
        return Dataset.from_list(formatted_data)
        
    def fine_tune(self, training_data: List[Dict], output_dir: str = None):
        """Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LoRA"""
        
        if output_dir is None:
            output_dir = self.lora_dir
            
        print(f"ğŸ“ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ {len(training_data)} Ø¹ÙŠÙ†Ø©...")
        
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        dataset = self.prepare_training_dataset(training_data)
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ LoRA config
        lora_config = LoraConfig(
            r=8,  # rank
            lora_alpha=16,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM"
        )
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ¯Ø±ÙŠØ¨
        if not hasattr(self.model, 'peft_config'):
            self.model = get_peft_model(self.model, lora_config)
            
        self.model.print_trainable_parameters()
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=3,
            per_device_train_batch_size=1,
            gradient_accumulation_steps=4,
            learning_rate=2e-4,
            logging_steps=10,
            save_steps=50,
            save_total_limit=2,
            fp16=False,  # CPU mode
            optim="adamw_torch",
            warmup_steps=10,
            report_to="none"
        )
        
        # Data collator
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False
        )
        
        # Tokenize dataset
        def tokenize_function(examples):
            return self.tokenizer(
                examples["text"],
                truncation=True,
                max_length=512,
                padding="max_length"
            )
            
        tokenized_dataset = dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=dataset.column_names
        )
        
        # Trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=tokenized_dataset,
            data_collator=data_collator
        )
        
        # Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        print("ğŸ”¥ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")
        trainer.train()
        
        # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        self.model.save_pretrained(output_dir)
        self.tokenizer.save_pretrained(output_dir)
        
        print(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ ÙÙŠ: {output_dir}")
        
    def should_trigger_training(self, conversation_count: int, threshold: int = 50) -> bool:
        """ØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ¬Ø¨ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
        return conversation_count > 0 and conversation_count % threshold == 0
